---
title: "Project_SocialMedia"
author: "Akshita"
date: "2024-04-27"
output: html_document
---

Dataset Name: Student Social Media Usage and Weekly Feelings

Description:

This dataset captures the social media usage habits of students along with their weekly feelings, mood, productivity, tiredness, learnings from social media, and interview/career-related interactions.
Variables:

Social Media Usage:

Instagram_Time: Time spent on Instagram (in hours per day)

Linkedin_Time: Time spent on LinkedIn (in hours per day)

Snapchat_Time: Time spent on Snapchat (in hours per day)

Twitter_Time: Time spent on Twitter (in hours per day)

Whatsapp_Time: Time spent on WhatsApp (in hours per day)

Youtube_Time: Time spent on YouTube (in hours per day)

OTT_Time: Time spent on Over-the-Top (OTT) platforms (in hours per day)

Reddit_Time: Time spent on Reddit (in hours per day)

Weekly_Feelings: Overall weekly feelings of students on a scale of 1 to 5.

Other Metrics:

Mood Productivity: Student's mood during the week (on a scale of 1 to 5)

Tiredness: Student's level of tiredness during the week (on a scale of 1 to 5)

Learnings: Amount of learning derived from social media usage (on a scale of 1 to 10)

Interview_Call: Binary variable indicating whether the student received an interview call or coffee chat (1 if yes, 0 if no)

Data Collection: The dataset was collected through surveys where students reported their daily social media usage and weekly feelings, mood, productivity, and tiredness. Additionally, students reported any interactions or learnings related to their career pursuits.

Use Case: The dataset can be used to analyze the relationship between social media usage patterns and students' well-being, mood, productivity, and career-related outcomes. It can also be used to explore the impact of different social media platforms on various aspects of students' lives.


Data Cleaning & Pre=Processing

```{r}
social_media <- read.csv("C:/Users/HP/Downloads/Social_media_original.csv")
str(social_media)
social_media_cleaned <- social_media[,-1]

#changing column names
change_cols_index <- c(2,4,6,8,10,12,14,16,17,18,19,20,21,22,23,24)
change_cols_name <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time", "Application Type", "Interview_call_received", "Networking", "Learning", "Mood_Productivity", "Morning_tireness", "Sleep_trouble", "Weekly_Feelings")
colnames(social_media_cleaned)[change_cols_index] <- change_cols_name


# Convert "NA", "N/A", "n/a", "na", "N.A", "n.a" to 0
social_media_cleaned[social_media_cleaned == "NA" | social_media_cleaned == "N/A" | social_media_cleaned == "na" | social_media_cleaned == "n/a" | social_media_cleaned == "N.A" | social_media_cleaned == "n.a" | social_media_cleaned == "0" | social_media_cleaned == ""] <- NA
social_media_cleaned[is.na(social_media_cleaned)] <- 0

# Define a function to convert time strings to decimal hours
convert_to_decimal_hours <- function(time_string) {
# Check if NA values are present
if (any(is.na(time_string))) {
         return(rep(NA, length(time_string)))  # Return NA for NA values
     }
     
# Define a function to convert HH:MM format to decimal hours
     hhmm_to_decimal <- function(hhmm) {
         parts <- as.numeric(strsplit(hhmm, ":")[[1]])  # Split into hours and minutes
         hours <- parts[1]
         minutes <- ifelse(length(parts) > 1, parts[2], 0)  # Handle missing minutes
         total_hours <- hours + minutes / 60
         return(total_hours)
     }
     
# Convert time strings to decimal hours
decimal_hours <- sapply(time_string, function(x) {
         if (grepl("^\\d+:\\d+$", x)) {
             return(hhmm_to_decimal(x))  # Convert HH:MM format
         } else if (grepl("^\\d+\\.\\d+$", x)) {
             return(as.numeric(x))  # Convert decimal format
         } else if (grepl("^\\d+$", x)) {
             return(as.numeric(x))  # Convert whole numbers
         } else {
             return(NA)  # Return NA for other cases
         }
     })
     
     return(decimal_hours)
}

time_columns <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time") 
# Apply the conversion function to all time columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], convert_to_decimal_hours)

#Treating NA in whatsapp column
mean_value <- mean(social_media_cleaned$Whatsapp_Time, na.rm = TRUE)
social_media_cleaned$Whatsapp_Time[is.na(social_media_cleaned$Whatsapp_Time)] <- mean_value

# Verify the result
str(social_media_cleaned)

# Convert 'No' and 'Yes' to 0 and 1 
social_media_cleaned$Mood_Productivity <- ifelse(social_media_cleaned$Mood_Productivity == "No", 0, 1)

social_media_cleaned$Morning_tireness <- ifelse(social_media_cleaned$Morning_tireness == "No", 0, 1)

social_media_cleaned$Sleep_trouble <- ifelse(social_media_cleaned$Sleep_trouble == "No", 0, 1)

# Select the relevant columns
time_columns <- social_media_cleaned[, grepl("_Time", colnames(social_media_cleaned))]
other_columns <- social_media_cleaned[c("Weekly_Feelings")]
 
# Combine the columns
combined_data <- cbind(time_columns, other_columns)

# Check the structure of the dataset
str(combined_data)
summary(combined_data)
```

Visualizing

```{r}
library(ggplot2)

# Create a boxplot for Instagram_Time by Feelings_Category
ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Instagram_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Instagram Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Linkedin_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Linkedin Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Snapchat_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Snapchat Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Twitter_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Twitter Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Whatsapp_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Whatsapp Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Youtube_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Youtube Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = OTT_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "OTT Time") +
     theme_minimal()

ggplot(data = combined_data, aes(x = Weekly_Feelings, y = Reddit_Time, fill = Weekly_Feelings)) +
     geom_boxplot() +
     labs(title = "Boxplot of Instagram Time by Weekly_Feelings",
          x = "Weekly_Feelings",
          y = "Reddit Time") +
     theme_minimal()

# Exclude the 'Weekly_Feelings' column before calculating the correlation matrix
numeric_data <- combined_data[, -which(names(combined_data) == "Weekly_Feelings")]

# Calculate the correlation matrix
correlation_matrix <- cor(numeric_data)
print(correlation_matrix)

# Create a heatmap for the correlation matrix
heatmap(correlation_matrix,
        symm = TRUE,        # Display symmetrically
        main = "Correlation Heatmap of Social Media Usage Variables",
        xlab = "Social Media Usage Variables",
        ylab = "Social Media Usage Variables",
        col = colorRampPalette(c("blue", "white", "red"))(100), # Choose a color palette
        scale = "none")     # Add a color scale
        
```


```{r}
# Scale the variables
scaled_data <- scale(combined_data[, -c(9)])  # Exclude the first and last columns (ID and Feelings_Category)

#comparing original and scaled values
# Plot histograms for original data
par(mfrow=c(2, ncol(combined_data) - 1))  # Set up plotting layout

for (i in 1:(ncol(combined_data) - 1)) {  # Exclude the last column (Feelings_Category)
  hist(combined_data[, i], main = paste("Original", names(combined_data)[i]), xlab = names(combined_data)[i], col = "lightblue")
}

# Plot histograms for scaled data
for (i in 1:(ncol(scaled_data))) {
  hist(scaled_data[, i], main = paste("Scaled", names(scaled_data)[i]), xlab = names(scaled_data)[i], col = "lightgreen")
}

```

EDA

```{r}
# Install and load the psych package
library(psych)

# Perform PCA
pca_result <- prcomp(scaled_data)
print(pca_result)
summary(pca_result)

# Scree plot
plot(pca_result, type = "l", main = "Scree Plot")

# Biplot
biplot(pca_result, cex = 0.8)

# Correlation
pairs.panels(combined_data[, -ncol(combined_data)],
              gap = 0,
              bg = c("red", "blue")[combined_data$Feelings_Category],
              pch=21)
 
pairs.panels(pca_result$x,
              gap=0,
              bg = c("red", "blue")[combined_data$Feelings_Category],
              pch=21)
```

Variance explained by each Principal Component (PC):

PC1 explains the highest proportion of variance at 36.47%, followed by PC2 with 22.09%.

The cumulative proportion of variance explained by PC1 and PC2 is 58.55%, indicating that these two components capture a significant amount of the overall variability in the data.

Subsequent components contribute progressively less to the total variance.

Interpretation of Principal Components:

PC1: It seems to be heavily influenced by variables like Instagram_Time, Linkedin_Time, Snapchat_Time, and Whatsapp_Time, as they have relatively high loadings on this component. This suggests that PC1 captures the overall usage pattern across these social media platforms.

PC2: Variables like Twitter_Time, OTT_Time, and Reddit_Time have higher loadings on this component. PC2 might represent a different aspect of social media usage or engagement compared to PC1.

Loadings of Variables on Principal Components:

Loadings represent the correlation between each variable and the principal component. Higher absolute values of loadings indicate stronger associations.

For example, Instagram_Time has a high positive loading on PC1, indicating that higher Instagram usage contributes significantly to this component.

Interpretation of PC Rotation:

Rotation provides insight into the relationship between original variables and principal components.

For instance, the negative loading of Reddit_Time on PC1 suggests an inverse relationship between Reddit usage and the overall social media usage pattern captured by PC1.

```{r}

# Perform Factor Analysis
fa_result <- fa(scaled_data, nfactors = 3, rotate = "varimax")
print(fa_result)

# Summary of Factor Analysis
summary(fa_result)

# Biplot of factor loadings
biplot(fa_result)

# Factor loadings plot
factor_loading_plot <- fa.diagram(fa_result, main = "Factor Loadings Plot")

#1. Kaiser's Criterion:
# Extract eigenvalues from FA results
eigenvalues <- fa_result$values
print(eigenvalues)

# Retain factors with eigenvalues greater than 1
num_factors_kaiser <- sum(eigenvalues > 1)
print(num_factors_kaiser)

# Factor loadings
fa_loadings <- fa_result$loadings
# Interpret factor loadings
print(fa_loadings)

# Access communalities
communalities <- fa_result$communality
# Interpret communalities
print(communalities)

```

Factor Loadings:

Each variable's loading on each factor indicates the strength and direction of its relationship with that factor.
For example, in Factor 1 (MR1), variables like Instagram_Time, Snapchat_Time, and Whatsapp_Time have high loadings, suggesting they are strongly associated with this factor.

Factor 2 (MR2) shows high loadings for Twitter_Time, OTT_Time, and Reddit_Time, indicating a different set of variables strongly associated with this factor.

Factor 3 (MR3) has high loadings for Youtube_Time, indicating its strong association with this factor.

Eigenvalues and Kaiser's Criterion:

Eigenvalues represent the variance explained by each factor. Factors with eigenvalues greater than 1 are typically retained, suggesting they explain more variance than individual variables.

In this case, Kaiser's Criterion suggests retaining two factors, as they have eigenvalues greater than 1.

Communalities:

Communalities represent the proportion of variance in each variable that is accounted for by the extracted factors.

Higher communalities indicate that a larger portion of the variance in the variable is explained by the factors.

For example, variables like Instagram_Time and Youtube_Time have high communalities, indicating that a significant portion of their variance is explained by the extracted factors.

```{r}
library(cluster)
# Perform hierarchical clustering
dist_matrix <- dist(scaled_data)
colnames(dist_matrix) <- rownames(dist_matrix)
print(dist_matrix)

# Clustering
hclust_result <- hclust(dist_matrix, method = "ward.D2")
plot(hclust_result, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. ward D2 linkage")

# Invoking hclust command (cluster analysis by single linkage method)      
clus_sm.1 <- hclust(dist_matrix, method = "single")
plot(clus_sm.1, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Nearest neighbor linkage")

#Default - Complete
clus_sm.2 <- hclust(dist_matrix)
plot(clus_sm.2,hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Farthest neighbor linkage")


#Average
clus_sm.3 <- hclust(dist_matrix,method="average")
plot(clus_sm.3,hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Group average linkage")

# We will use agnes function as it allows us to select option for data standardization, the distance measure and clustering algorithm in one single function
(agn.sm <- agnes(scaled_data, metric="euclidean", stand=TRUE, method = "single"))
#  Description of cluster merging
agn.sm$merge

#Interactive Plots
plot(agn.sm, which.plots=1)
plot(agn.sm, which.plots=2)

# Cut the dendrogram to obtain clusters
num_clusters <- 3  # Choose the number of clusters
clusters <- cutree(hclust_result, k = num_clusters)

# Visualize the dendrogram
plot(hclust_result, main = "Dendrogram of Hierarchical Clustering")
rect.hclust(hclust_result, k = num_clusters, border = 2:5)

# K-means, k=2, 3, 4, 5, 6
# Centers (k's) are numbers thus, 10 random sets are chosen

# Computing the percentage of variation accounted for. Two clusters
(kmeans2.sm <- kmeans(scaled_data,2,nstart = 10))
# Computing the percentage of variation accounted for. Two clusters
perc.var.2 <- round(100*(1 - kmeans2.sm$betweenss/kmeans2.sm$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2

# Computing the percentage of variation accounted for. Three clusters
(kmeans3.sm <- kmeans(scaled_data,3,nstart = 10))
# Computing the percentage of variation accounted for. Two clusters
perc.var.3 <- round(100*(1 - kmeans3.sm$betweenss/kmeans3.sm$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3

# Computing the percentage of variation accounted for. Four clusters
(kmeans4.sm <- kmeans(scaled_data,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.sm$betweenss/kmeans4.sm$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4

# Computing the percentage of variation accounted for. Five clusters
(kmeans5.sm <- kmeans(scaled_data,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.sm$betweenss/kmeans5.sm$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5

# Computing the percentage of variation accounted for. Six clusters
(kmeans6.sm <- kmeans(scaled_data,6,nstart = 10))
perc.var.6 <- round(100*(1 - kmeans6.sm$betweenss/kmeans6.sm$totss),1)
names(perc.var.6) <- "Perc. 6 clus"
perc.var.6

Variance_List <- c(perc.var.2,perc.var.3,perc.var.4,perc.var.5,perc.var.6)
Variance_List
plot(Variance_List)

# Plot clusters
plot(scaled_data, col = kmeans2.sm$cluster, 
      main = "K-means Clustering with 2 Clusters", 
      xlab = "X-axis Label", ylab = "Y-axis Label")
 
# Add cluster centers to the plot
points(kmeans2.sm$centers, col = 1:2, pch = 8, cex = 2)
 
# Add legend
legend("topright", legend = unique(kmeans2.sm$cluster), 
        col = 1:2, pch = 8, title = "Cluster")
```

The agglomerative coefficient of 0.631 suggests moderate clustering structure in the data.

For K-means with 2 clusters, approximately 76.5% of the variation in the data is accounted for by the clustering solution.

Increasing the number of clusters to 3 slightly reduces the percentage of variation accounted for (60.3%), indicating that the additional cluster captures some of the remaining variance but at a diminishing rate.

Further increasing the number of clusters to 4, 5, and 6 continues to capture more variance but with diminishing returns, as reflected in the decreasing percentages of variation accounted for (44.8%, 33.4%, and 27.3% respectively).

Models:
Multiple Linear Regression:

```{r}
sm_model_fit <- lm(Weekly_Feelings ~ ., data = combined_data)
#show the results
summary(sm_model_fit)
#Summary has three sections. Section1: How well does the model fit the data (before Coefficients). Section2: Is the hypothesis supported? (until sifnif codes). Section3: How well does data fit the model (again).
# Useful Helper Functions
coefficients(sm_model_fit)

predictor_vars <- c("Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time")
for (predictor in predictor_vars) {
  plot(Weekly_Feelings ~ get(predictor), data = combined_data, 
       main = paste("Regression Line for Weekly Feelings and", predictor))
  abline(lm(Weekly_Feelings ~ get(predictor), data = combined_data), col = "blue")
}

# Reset plotting parameters
par(mfrow = c(1, 1))

# Diagnostic plots for the linear regression model
plot(sm_model_fit)

# Assess the acceptance of the model
# Check R-squared and adjusted R-squared
r_squared <- summary(sm_model_fit)$r.squared
adj_r_squared <- summary(sm_model_fit)$adj.r.squared
cat("R-squared:", r_squared, "\n")
cat("Adjusted R-squared:", adj_r_squared, "\n")

# Check the F-statistic and its p-value
f_statistic <- summary(sm_model_fit)$fstatistic
f_statistic_value <- f_statistic[1]
f_statistic_p_value <- pf(f_statistic_value, f_statistic[2], f_statistic[3], lower.tail = FALSE)
cat("F-statistic:", f_statistic_value, "\n")
cat("p-value:", f_statistic_p_value, "\n")

# Plot pairs
pairs(combined_data, main = "Combined Data")
confint(sm_model_fit,level=0.95)
fitted(sm_model_fit)
residuals(sm_model_fit)

# Plot residuals vs. fitted values
plot(sm_model_fit, which = 1)

# Plot normal Q-Q plot of residuals
plot(sm_model_fit, which = 2)

# Make predictions
predictions <- predict(sm_model_fit, newdata = combined_data)

# Print predictions
print(predictions)

# Calculate Mean Squared Error (MSE)
mse <- mean(sm_model_fit$residuals^2)

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(sm_model_fit$residuals))

# Print the accuracy metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

The model's R-squared value is 0.3682, indicating that approximately 36.82% of the variance in the dependent variable (Weekly Feelings) is explained by the independent variables in the model.

However, the adjusted R-squared value is negative (-0.05306), suggesting that the model may not be adequately capturing the underlying relationships in the data, especially when considering the number of predictors in the model.

Only the coefficient for the intercept is statistically significant at the 0.05 level, with a p-value of 0.000163.

2. Logistic Regression:
```{r}

# Create a binary outcome variable
combined_data$Feelings_Category <- ifelse(combined_data$Weekly_Feelings > 3, "High", "Low")

# Convert Feelings_Category to factor
combined_data$Feelings_Category <- factor(combined_data$Feelings_Category, levels = c("Low", "High"))

# Drop Weekly_Feelings variable using subset function
combined_data <- subset(combined_data, select = -Weekly_Feelings)

# Check the structure of the dataset
str(combined_data)

# Fit logistic regression model
logistic_model <- glm(Feelings_Category ~ ., data = combined_data, family = binomial)

# Summary of the model
summary(logistic_model)

# Check p-values
p_values <- summary(logistic_model)$coefficients[, 4]
print(p_values)

# Check AIC and BIC
AIC_value <- AIC(logistic_model)
print(AIC_value)
BIC_value <- BIC(logistic_model)
print(BIC_value)

# Compare null deviance and residual deviance
null_deviance <- summary(logistic_model)$null.deviance
residual_deviance <- summary(logistic_model)$deviance
deviance_reduction <- null_deviance - residual_deviance
print(paste("Null Deviance:", null_deviance))
print(paste("Residual Deviance:", residual_deviance))
print(paste("Deviance Reduction:", deviance_reduction))

# Assumptions Checking (Diagnostic Plots)
par(mfrow = c(2, 2))  # Set up a 2x2 plot layout
plot(logistic_model)

# Plot residuals
plot(resid(logistic_model), ylab = "Residuals")

# Predicting probabilities
predicted_probs <- predict(logistic_model, type = "response")

# Model Accuracy
# Create a confusion matrix
predicted_classes <- ifelse(predicted_probs > 0.5, "High", "Low")
conf_matrix <- table(predicted_classes, combined_data$Feelings_Category)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(accuracy)

# Load required library
library(pROC)

# Calculate ROC curve
roc_curve <- roc(combined_data$Feelings_Category, predicted_probs)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

# Add diagonal reference line
abline(a = 0, b = 1, lty = 2, col = "red")

# Add AUC to the plot
auc_value <- auc(roc_curve)
text(0.8, 0.2, paste("AUC =", round(auc_value, 2)), col = "black")

```

The coefficients represent the log odds of the outcome variable (Feelings_Category) for a one-unit increase in each predictor variable.

The p-values associated with each coefficient indicate the statistical significance of the predictors.

Only "Linkedin_Time" has a p-value less than 0.1, suggesting potential significance at the 0.1 significance level.

The null deviance (27.91) represents the deviance when only the intercept is included in the model.

The residual deviance (17.93) represents the deviance after fitting the logistic regression model.

3. LDA:

```{r}
library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)

str(combined_data)

# See correlation
#Extract independent variables
independent_vars <- combined_data[, -ncol(combined_data)] # Exclude last column (dependent variable)
correlation_matrix <- cor(independent_vars)
print(correlation_matrix)
 
# Create scatter plot matrix
pairs(independent_vars, 
       main = "Scatter Plot Matrix", pch = 19) # Set point shape

#Based on relvent columns
# Subset the combined_data dataset
subset_data <- combined_data[, c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Feelings_Category")]

# Print the first few rows of the subsetted data to verify
head(subset_data)
str(subset_data)

subset_data_independent <- as.matrix(subset_data[,c(1:5)])
subset_data_raw <- cbind(subset_data_independent, as.numeric(as.factor(subset_data$Feelings_Category))-1)
colnames(subset_data_raw)[6] <- "Feelings_Category"

smp_size_raw <- floor(0.75 * nrow(subset_data_raw))
train_ind_raw <- sample(nrow(subset_data_raw), size = smp_size_raw)
train_raw.df <- as.data.frame(subset_data_raw[train_ind_raw, ])
test_raw.df <- as.data.frame(subset_data_raw[-train_ind_raw, ])

subset_data_raw.lda <- lda(formula = train_raw.df$Feelings_Category ~ ., data = train_raw.df)
subset_data_raw.lda

summary(subset_data_raw.lda)
print(subset_data_raw.lda)
plot(subset_data_raw.lda)

subset_data_raw.lda.predict <- predict(subset_data_raw.lda, newdata = test_raw.df)
subset_data_raw.lda.predict$class
subset_data_raw.lda.predict$x

# Get the posteriors as a dataframe.
subset_data_raw.lda.predict.posteriors <- as.data.frame(subset_data_raw.lda.predict$posterior)

pred <- prediction(subset_data_raw.lda.predict.posteriors[,2], test_raw.df$Feelings_Category)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


# do a quick plot to understand how good the model is
plot(subset_data_raw.lda, col = as.integer(train_raw.df$Feelings_Category))
# Sometime bell curves are better
#plot(subset_data_raw.lda, dimen = 1, type = "b")
# THis plot shows the essense of LDA. It puts everything on a line and finds cutoffs. 
# Partition plots
#partimat(Species ~ Weigth + Sepal.Width + Length1 + Length2 + Length3 + Height + Width, data=train_raw.df_new, method="lda")

# Lets focus on accuracy. Table function
lda.train <- predict(subset_data_raw.lda)
train_raw.df$lda <- lda.train$class
table(train_raw.df$lda,train_raw.df$Feelings_Category)
# running accuracy on the training set shows how good the model is. It is not an indication of "true" accuracy. We will use the test set to approximate accuracy
lda.test <- predict(subset_data_raw.lda,test_raw.df)
test_raw.df$lda <- lda.test$class
table(test_raw.df$lda,test_raw.df$Feelings_Category)

> accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
> accuracy
[1 1
```

Some variables have moderate to strong correlations with each other, which may indicate multicollinearity. For example, "Instagram_Time" and "Snapchat_Time" have a correlation coefficient of 0.76.

From the LDA output, we can see the coefficients of linear discriminants (LD), which indicate the contribution of each independent variable to the discriminant function. For instance, "Linkedin_Time" has a coefficient of 0.601 in LD1, suggesting its strong influence on class separation.

The plot of the ROC curve, along with the calculated AUC, provides insights into the model's ability to distinguish between classes. For example, an AUC of 0.75 indicates that the model has fairly good discriminatory power.

The confusion matrices show the counts of correct and incorrect predictions on both the training and test datasets. For example, in the test set, there were 4 correct predictions for class 0 and 1 correct prediction for class 1 out of a total of 7 observations.

#Learning and Takeaways

PCA may reveal insights into how different social media platforms are used by students, which platforms are correlated, and which have the most significant impact on overall usage patterns.

FA can address multicollinearity issues in social media data by extracting common variance shared among variables into latent factors, which can improve the stability and interpretability of subsequent analyses.

Analyzing clusters can provide insights into the characteristics and dynamics of each user group, such as common interests, engagement levels, or responses to content, facilitating data-driven decision-making in social media management.

LM coefficients can help identify which social media platforms or usage behaviors have a significant impact on students' weekly feelings or other outcomes, guiding interventions or recommendations for healthier social media habits.

Checking LDA assumptions (e.g., normality, equal covariance matrices) ensures the reliability of the classification results and the interpretability of discriminant functions, guiding the appropriate use of LDA in social media analysis.

